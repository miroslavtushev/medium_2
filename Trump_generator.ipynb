{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"tweets_01-08-2021.csv\", index_col=\"id\", parse_dates=['date'])\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace({'isRetweet' : {'f' : 0, 't' : 1},\n",
    "             'isDeleted' : {'f' : 0, 't' : 1},\n",
    "             'isFlagged' : {'f' : 0, 't' : 1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "text = data.text.to_list()\n",
    "processed_tweets = []\n",
    "tokenizer = RegexpTokenizer('\\w+|\\S+')\n",
    "for tweet in text:\n",
    "    tweet = re.sub('(https?:[\\w\\/\\.\\d]+)|…|(^RT)|“|”|\"', \"\", tweet)\n",
    "    tweet = re.sub(\"&amp;?\", \"and\", tweet)\n",
    "    processed_tweets.append(tokenizer.tokenize(tweet.lower()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[6].text.split()\n",
    "#processed_tweets[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kdnuggets.com/2019/11/markov-chains-train-text-generation.html\n",
    "\n",
    "# chains of 2 words\n",
    "k = 2\n",
    "# list to hold those chains\n",
    "sets_of_k_words = []\n",
    "\n",
    "# create chains with k-length\n",
    "for tweet in processed_tweets:\n",
    "    sets_of_k_words.append([' '.join(tweet[i:i+k]) for i, _ in enumerate(tweet[:-k+1])])\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "# how many unique chains are there?\n",
    "distinct_sets_of_k_words = list(set([chain for s in sets_of_k_words for chain in s]))\n",
    "# how many unique words are there?\n",
    "distinct_words = list(set([elem for l in processed_tweets for elem in l]))\n",
    "# sparse matrix: rows - k-length sequence, columns - all possible words in tweets\n",
    "next_after_k_words_matrix = dok_matrix((len(distinct_sets_of_k_words), len(distinct_words)), dtype=np.uint16)\n",
    "\n",
    "# to look up the index of a chain (row) for the matrix\n",
    "k_words_idx_dict = {word: i for i, word in enumerate(distinct_sets_of_k_words)}\n",
    "# to look up the index of a word (column) for the matrix\n",
    "word_idx_dict = {word: i for i, word in enumerate(distinct_words)}\n",
    "\n",
    "# for each sequence go over all tweets and find the next word\n",
    "# increment the count for that word\n",
    "for i, set_of_k_words in enumerate(sets_of_k_words):\n",
    "    for j, k_word in enumerate(set_of_k_words[:-k+1]):\n",
    "        # index for a row (chain)\n",
    "        word_sequence_idx = k_words_idx_dict[k_word]\n",
    "        # get the index for next w\n",
    "        next_word_idx = word_idx_dict[processed_tweets[i][j+k]]\n",
    "        next_after_k_words_matrix[word_sequence_idx, next_word_idx] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple case with 0 alpha and fixed length\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from numpy.random import choice\n",
    "\n",
    "def sample_next_word_after_sequence(word_sequence, alpha = 0):\n",
    "    next_word_vector = next_after_k_words_matrix[k_words_idx_dict[word_sequence]] + alpha\n",
    "    likelihoods = csr_matrix(next_word_vector)/next_word_vector.sum()\n",
    "    weights = likelihoods.toarray().flatten()\n",
    "    # if no words possible - terminate\n",
    "    if weights.sum() == 0.0:\n",
    "        return \"\"\n",
    "    return choice(distinct_words, p=weights)\n",
    "    \n",
    "def stochastic_chain(seed, chain_length=10, seed_length=k):\n",
    "    current_words = seed.split(' ')\n",
    "    if len(current_words) != seed_length:\n",
    "        raise ValueError(f'wrong number of words, expected {seed_length}')\n",
    "    sentence = seed\n",
    "\n",
    "    for _ in range(chain_length):\n",
    "        sentence += ' '\n",
    "        next_word = sample_next_word_after_sequence(' '.join(current_words))\n",
    "        if next_word == \"\":\n",
    "            return sentence\n",
    "        sentence += next_word\n",
    "        current_words = current_words[1:]+[next_word]\n",
    "    return sentence\n",
    "  \n",
    "stochastic_chain(choice(distinct_sets_of_k_words), chain_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating CDF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lens = np.sort([len(tweet) for tweet in processed_tweets])\n",
    "print(lens[-1])\n",
    "\n",
    "def gen_prob(val, lens=lens):\n",
    "    for i, elem in enumerate(lens):\n",
    "        if elem >= val:\n",
    "            return i / len(lens)\n",
    "    return 1.00\n",
    " \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.cumsum(lens) / np.cumsum(lens)[-1])\n",
    "ax.set_title(\"CDF for tweet length\")\n",
    "ax.set_xlim(0, len(lens))\n",
    "ax.set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import random\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sample_next_word_after_sequence(word_sequence, alpha = 0):\n",
    "    # generate a random word by chance\n",
    "    if random() <= alpha:\n",
    "        return distinct_words[choice(len(distinct_words)-1)]\n",
    "    next_word_vector = next_after_k_words_matrix[k_words_idx_dict[word_sequence]]\n",
    "    likelihoods = csr_matrix(next_word_vector)/next_word_vector.sum()\n",
    "    weights = likelihoods.toarray().flatten()\n",
    "    if weights.sum() == 0.0:\n",
    "        return \"\"\n",
    "    return choice(distinct_words, p=weights)\n",
    "\n",
    "def stochastic_chain(seed, alpha=0):\n",
    "    # if only 1 word provided\n",
    "    if len(seed.split(' ')) != k:\n",
    "        # complete the chain\n",
    "        possible_words = [s for s in distinct_sets_of_k_words if s.startswith(seed)]\n",
    "        seed = choice(possible_words)           \n",
    "    current_words = seed.split(' ')  \n",
    "    sentence = seed\n",
    "\n",
    "    while(1):\n",
    "        sentence += ' '\n",
    "        next_word = sample_next_word_after_sequence(' '.join(current_words), alpha)\n",
    "        if next_word == \"\":\n",
    "            return postprocess(sentence)\n",
    "        elif next_word in list(\".!?\"):\n",
    "            sentence += next_word\n",
    "            if random() <= gen_prob(len(sentence.split())):\n",
    "                return postprocess(sentence)\n",
    "        else:\n",
    "            sentence += next_word\n",
    "        current_words = current_words[1:]+[next_word]\n",
    "        \n",
    "def postprocess(sent):\n",
    "    final = []\n",
    "    for s in sent_tokenize(sent):\n",
    "        final.append(re.sub(r\" ([.?!])$\", r\"\\1\", s).capitalize())\n",
    "    return \" \".join(final)\n",
    "        \n",
    "stochastic_chain('twitter', alpha=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funny ones k=2\n",
    "'@ojmart thank you @stevedaines for being named , by virtue of the debate . no better person for the people ! love the state of michigan .'\n",
    "'wonderful president was greatly helped by tariffs from china . we will hopefully come through with his candidacy ! #maga #imwithyou '\n",
    "'Democrats pushing the radical left! #sc01'\n",
    "'Obama fair and square. 5 :30 pm est on @nbc. Enjoy! @foxnews just wrote a story on women.'\n",
    "'Obama betrays israel yet again! #trump2016 ~ @usplaymoney @realdonaldtrump @beny_benson'\n",
    "'Republicans prepare to #kag!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# k=3\n",
    "\"After 2 years of action – it ’s a primary record for a sitting president , you 're off to a good provider\"\n",
    "'Forget that joe biden did in 47 years. A vicious killer who destroyed so many great things ,he has woke america up and people are talking about.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k =4\n",
    "'Pelosi says she got set up by the obama administration.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
